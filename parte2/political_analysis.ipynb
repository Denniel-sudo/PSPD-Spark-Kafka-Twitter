{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f509dd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.1.1)\n",
      "Requirement already satisfied: wordcloud in /home/denni/.local/lib/python3.10/site-packages (1.8.2.2)\n",
      "Requirement already satisfied: matplotlib in /home/denni/.local/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: numpy in /home/denni/.local/lib/python3.10/site-packages (1.23.3)\n",
      "Requirement already satisfied: textblob in /home/denni/.local/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in /home/denni/.local/lib/python3.10/site-packages (from pyspark) (0.10.9)\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (from wordcloud) (9.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/denni/.local/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/denni/.local/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/denni/.local/lib/python3.10/site-packages (from matplotlib) (4.37.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/denni/.local/lib/python3.10/site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: joblib in /home/denni/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /home/denni/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/denni/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2022.9.13)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark wordcloud matplotlib numpy textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a26f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5943d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf14f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/denni/.ivy2/cache\n",
      "The jars for the packages stored in: /home/denni/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-65d38db4-60b4-4784-9027-c9aaaa65b1df;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.1.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.1 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1168ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.1 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.1.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-65d38db4-60b4-4784-9027-c9aaaa65b1df\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/20ms)\n",
      "22/09/22 20:42:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"spark://192.168.56.110:7077\") \\\n",
    "        .config(\"spark.sql.streaming.schemaInference\", True) \\\n",
    "        .appName(\"Sentiment Analysis Spark\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n",
    "\n",
    "# To always shoe the results of Dataframes and improve the formatting output\n",
    "spark.conf.set(\"spark.sql.repl.eagerVal.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a18aa0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      " |-- headers: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key: string (nullable = true)\n",
      " |    |    |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"192.168.56.110:9092\") \\\n",
    "    .option(\"subscribe\", \"twitter-data\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"includeHeaders\", \"true\") \\\n",
    "    .load()\n",
    "\n",
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "950e379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def preprocessing(tweets):\n",
    "    df = tweets.select(col(\"value\").alias(\"text\"))\n",
    "    df = df.na.replace('', None)\n",
    "    df = df.na.drop()\n",
    "    df = df.withColumn('text', regexp_replace('text', r'http\\S+', ''))\n",
    "    df = df.withColumn('text', regexp_replace('text', '#', ''))\n",
    "    df = df.withColumn('text', regexp_replace('text', 'RT', ''))\n",
    "    df = df.withColumn('text', regexp_replace('text', ':', ''))\n",
    "                       \n",
    "    return df\n",
    "def sentiment_analysis(text):\n",
    "#     translated = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "    analysis = TextBlob(text).sentiment.polarity\n",
    "    if analysis > 0:\n",
    "        return 1\n",
    "    else:               \n",
    "        return 0 \n",
    "                             \n",
    "def text_classification(df):\n",
    "    sentiment_analysis_udf = udf(sentiment_analysis, StringType())\n",
    "    df = df.withColumn(\"label\", sentiment_analysis_udf(\"text\"))\n",
    "                       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb99186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forEachBatch(df, df_id):\n",
    "    df = preprocessing(df)\n",
    "    df = text_classification(df)\n",
    "    \n",
    "    data = df.withColumn(\"label\", col(\"label\").cast(IntegerType()))\n",
    "    \n",
    "    divided_data = data.randomSplit([0.7, 0.3])\n",
    "    training_data = divided_data[0].na.drop()\n",
    "    testing_data = divided_data[1].na.drop()\n",
    "\n",
    "    train_rows = training_data.count()\n",
    "    test_rows = testing_data.count()\n",
    "    \n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"SentimentWords\")\n",
    "    tokenized_train = tokenizer.transform(training_data)\n",
    "                         \n",
    "    swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(),\n",
    "                        outputCol=\"MeaningfulWords\")\n",
    "    sw_removed_train = swr.transform(tokenized_train)\n",
    "                         \n",
    "    hash_tf = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "    numeric_train_data = hash_tf.transform(sw_removed_train).select(\"label\",\"MeaningfulWords\", \"features\")\n",
    "                         \n",
    "                         \n",
    "    lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10, regParam=0.01)\n",
    "    model = lr.fit(numeric_train_data)\n",
    "    print(\"Training is done!\")\n",
    "\n",
    "    tokenized_test = tokenizer.transform(testing_data)\n",
    "    sw_removed_test = swr.transform(tokenized_test)\n",
    "    numeric_test = hash_tf.transform(sw_removed_test).select(\n",
    "        'label','MeaningfulWords', 'features' \n",
    "    )\n",
    "\n",
    "    prediction = model.transform(numeric_test)\n",
    "    prediction_final = prediction.select(\"MeaningfulWords\", \"prediction\", \"label\")\n",
    "\n",
    "    approve_prediction = prediction_final.filter(\n",
    "        prediction_final['prediction'] == 1\n",
    "    ).count()\n",
    "    \n",
    "    reprove_prediction = prediction_final.filter(\n",
    "        prediction_final['prediction'] == 0\n",
    "    ).count()\n",
    "    \n",
    "    correct_prediction = prediction_final.filter(\n",
    "        prediction_final['prediction'] == prediction_final['label']).count()\n",
    "    total_data = prediction_final.count()\n",
    "\n",
    "    print(\"correct prediction:\", correct_prediction, \"total data:\", total_data, \n",
    "         \", accuracy:\", correct_prediction/total_data * 100, \"%\")\n",
    "    \n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    print(\"LULA:\")\n",
    "    print(\"Aprovacao: \", format(approve_prediction/total_data * 100, \".2f\"), \"%\")\n",
    "    print(\"Reprovacao: \", format(reprove_prediction/total_data * 100, \".2f\"), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaa6c6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/22 20:42:13 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4365cc5d-7d2a-4b54-ba75-e692bfab51d3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/09/22 20:42:33 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/09/22 20:42:33 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 39 total data: 59 , accuracy: 66.10169491525424 %\n",
      "--------------------------------------------------------------\n",
      "LULA:\n",
      "Aprovacao:  18.64 %\n",
      "Reprovacao:  81.36 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/22 20:42:50 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 36804 milliseconds\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 5 total data: 12 , accuracy: 41.66666666666667 %\n",
      "--------------------------------------------------------------\n",
      "LULA:\n",
      "Aprovacao:  91.67 %\n",
      "Reprovacao:  8.33 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/22 20:43:02 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 11920 milliseconds\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/22 20:43:14 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 11697 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 2 total data: 6 , accuracy: 33.33333333333333 %\n",
      "--------------------------------------------------------------\n",
      "LULA:\n",
      "Aprovacao:  0.00 %\n",
      "Reprovacao:  100.00 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 76:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 88:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 4 total data: 7 , accuracy: 57.14285714285714 %\n",
      "--------------------------------------------------------------\n",
      "LULA:\n",
      "Aprovacao:  0.00 %\n",
      "Reprovacao:  100.00 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 4 total data: 4 , accuracy: 100.0 %\n",
      "--------------------------------------------------------------\n",
      "LULA:\n",
      "Aprovacao:  25.00 %\n",
      "Reprovacao:  75.00 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/22 20:43:35 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 11181 milliseconds\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 138:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 6 total data: 9 , accuracy: 66.66666666666666 %\n",
      "--------------------------------------------------------------\n",
      "LULA:\n",
      "Aprovacao:  33.33 %\n",
      "Reprovacao:  66.67 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 149:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 160:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 0 total data: 5 , accuracy: 0.0 %\n",
      "--------------------------------------------------------------\n",
      "LULA:\n",
      "Aprovacao:  0.00 %\n",
      "Reprovacao:  100.00 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n",
      "correct prediction: 3 total data: 4 , accuracy: 75.0 %\n",
      "--------------------------------------------------------------\n",
      "LULA:\n",
      "Aprovacao:  0.00 %\n",
      "Reprovacao:  100.00 %\n"
     ]
    }
   ],
   "source": [
    "# Read the data stream. \n",
    "try:\n",
    "    query_stream_memory = tweets \\\n",
    "        .writeStream \\\n",
    "        .foreachBatch(forEachBatch) \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .start()\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Finalizando streaming...\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0dd0f6e9",
   "metadata": {},
   "source": [
    "# Stop the write Streams\n",
    "query_stream_memory.stop()\n",
    "\n",
    "# Stop the spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
