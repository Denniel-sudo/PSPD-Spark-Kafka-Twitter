{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f509dd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.1.1)\n",
      "Requirement already satisfied: wordcloud in /home/denni/.local/lib/python3.10/site-packages (1.8.2.2)\n",
      "Requirement already satisfied: matplotlib in /home/denni/.local/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: numpy in /home/denni/.local/lib/python3.10/site-packages (1.23.3)\n",
      "Requirement already satisfied: textblob in /home/denni/.local/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: deep_translator in /home/denni/.local/lib/python3.10/site-packages (1.8.3)\n",
      "Requirement already satisfied: py4j==0.10.9 in /home/denni/.local/lib/python3.10/site-packages (from pyspark) (0.10.9)\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (from wordcloud) (9.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/denni/.local/lib/python3.10/site-packages (from matplotlib) (4.37.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/denni/.local/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/denni/.local/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/denni/.local/lib/python3.10/site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep_translator) (4.11.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /home/denni/.local/lib/python3.10/site-packages (from deep_translator) (2.28.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.3.2.post1)\n",
      "Requirement already satisfied: tqdm in /home/denni/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: joblib in /home/denni/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/denni/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2022.9.13)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/denni/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark wordcloud matplotlib numpy textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a26f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5943d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbf14f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"spark://192.168.56.110:7077\") \\\n",
    "        .config(\"spark.sql.streaming.schemaInference\", True) \\\n",
    "        .appName(\"Sentiment Analysis Spark\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n",
    "\n",
    "# To always shoe the results of Dataframes and improve the formatting output\n",
    "spark.conf.set(\"spark.sql.repl.eagerVal.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a18aa0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      " |-- headers: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key: string (nullable = true)\n",
      " |    |    |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"192.168.56.110:9092\") \\\n",
    "    .option(\"subscribe\", \"twitter-data2\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"includeHeaders\", \"true\") \\\n",
    "    .load()\n",
    "\n",
    "tweets.printSchema()\n",
    "# lines = tweets.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\")\\\n",
    "#     .select(col(\"value\").alias(\"text\"))\n",
    "\n",
    "# lines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "950e379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def preprocessing(tweets):\n",
    "    df = tweets.select(col(\"value\").alias(\"text\"))\n",
    "    df = df.na.replace('', None)\n",
    "    df = df.na.drop()\n",
    "    df = df.withColumn('text', regexp_replace('text', r'http\\S+', ''))\n",
    "    df = df.withColumn('text', regexp_replace('text', '#', ''))\n",
    "    df = df.withColumn('text', regexp_replace('text', 'RT', ''))\n",
    "    df = df.withColumn('text', regexp_replace('text', ':', ''))\n",
    "                       \n",
    "    return df\n",
    "def sentiment_analysis(text):\n",
    "#     translated = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "    analysis = TextBlob(text).sentiment.polarity\n",
    "    if analysis > 0:\n",
    "        return 1\n",
    "    else:               \n",
    "        return 0 \n",
    "                             \n",
    "def text_classification(df):\n",
    "    sentiment_analysis_udf = udf(sentiment_analysis, StringType())\n",
    "    df = df.withColumn(\"label\", sentiment_analysis_udf(\"text\"))\n",
    "                       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb99186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forEachBatch(df, df_id):\n",
    "    ## Training Start Here\n",
    "    df = preprocessing(df)\n",
    "    df = text_classification(df)\n",
    "    \n",
    "    data = df.withColumn(\"label\", col(\"label\").cast(IntegerType()))\n",
    "    \n",
    "    divided_data = data.randomSplit([0.7, 0.3])\n",
    "    training_data = divided_data[0].na.drop()\n",
    "    testing_data = divided_data[1].na.drop()\n",
    "\n",
    "    train_rows = training_data.count()\n",
    "    test_rows = testing_data.count()\n",
    "    \n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"SentimentWords\")\n",
    "    tokenized_train = tokenizer.transform(training_data)\n",
    "#     tokenized_train.show(truncate=False, n=5)\n",
    "                         \n",
    "    swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(),\n",
    "                        outputCol=\"MeaningfulWords\")\n",
    "    sw_removed_train = swr.transform(tokenized_train)\n",
    "#     sw_removed_train.show(truncate=False, n=5)\n",
    "                         \n",
    "    hash_tf = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "    numeric_train_data = hash_tf.transform(sw_removed_train).select(\"label\",\"MeaningfulWords\", \"features\")\n",
    "#     numeric_train_data.show(truncate=False, n=5)\n",
    "                         \n",
    "                         \n",
    "    lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10, regParam=0.01)\n",
    "    model = lr.fit(numeric_train_data)\n",
    "    print(\"Training is done!\")\n",
    "\n",
    "    tokenized_test = tokenizer.transform(testing_data)\n",
    "    sw_removed_test = swr.transform(tokenized_test)\n",
    "    numeric_test = hash_tf.transform(sw_removed_test).select(\n",
    "        'label','MeaningfulWords', 'features' \n",
    "    )\n",
    "\n",
    "    prediction = model.transform(numeric_test)\n",
    "    prediction_final = prediction.select(\"MeaningfulWords\", \"prediction\", \"label\")\n",
    "#     prediction_final.show(truncate=False, n=4)\n",
    "\n",
    "    correct_prediction = prediction_final.filter(\n",
    "        prediction_final['prediction'] == prediction_final['label']).count()\n",
    "    total_data = prediction_final.count()\n",
    "\n",
    "    print(\"correct prediction:\", correct_prediction, \"total data:\", total_data, \n",
    "         \", accuracy:\", correct_prediction/total_data * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaa6c6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/19 01:23:08 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4efd00cf-fb15-4741-8cc8-2c40dfda0ae5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 38:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 6 total data: 8 , accuracy: 75.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/19 01:23:20 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 12129 milliseconds\n"
     ]
    }
   ],
   "source": [
    "# Read the data stream. \n",
    "try:\n",
    "    query_stream_memory = tweets \\\n",
    "        .writeStream \\\n",
    "        .foreachBatch(forEachBatch) \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .start()\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Finalizando streaming...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77224256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a7a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd61a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd210a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b62fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0cd34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26798548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0dd0f6e9",
   "metadata": {},
   "source": [
    "# Stop the write Streams\n",
    "query_stream_memory.stop()\n",
    "\n",
    "# Stop the spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
